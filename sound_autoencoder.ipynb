{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+EDgRjccp+PohEL2yEUV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/audio_synthesis/blob/main/sound_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0SinCpUI-3x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
        "    Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE:\n",
        "    \"\"\"\n",
        "    VAE represents a Deep Convolutional variational autoencoder architecture\n",
        "    with mirrored encoder and decoder components.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, #원래 이미지 기반인 vae를 오디오에 적용함\n",
        "                 input_shape,\n",
        "                 conv_filters,\n",
        "                 conv_kernels,\n",
        "                 conv_strides,\n",
        "                 latent_space_dim):\n",
        "        self.input_shape = input_shape # [28, 28, 1]\n",
        "        self.conv_filters = conv_filters # [2, 4, 8]\n",
        "        self.conv_kernels = conv_kernels # [3, 5, 3]\n",
        "        self.conv_strides = conv_strides # [1, 2, 2]\n",
        "        self.latent_space_dim = latent_space_dim # 2\n",
        "        self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.model = None\n",
        "\n",
        "        self._num_conv_layers = len(conv_filters)\n",
        "        self._shape_before_bottleneck = None\n",
        "        self._model_input = None\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def summary(self):\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, learning_rate=0.0001):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss=self._calculate_combined_loss,\n",
        "                           metrics=[self._calculate_reconstruction_loss,\n",
        "                                    self._calculate_kl_loss])\n",
        "\n",
        "    def train(self, x_train, batch_size, num_epochs):\n",
        "        self.model.fit(x_train,\n",
        "                       x_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=num_epochs,\n",
        "                       shuffle=True)\n",
        "\n",
        "    def save(self, save_folder=\".\"):\n",
        "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "        self._save_parameters(save_folder)\n",
        "        self._save_weights(save_folder)\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        latent_representations = self.encoder.predict(images)\n",
        "        reconstructed_images = self.decoder.predict(latent_representations)\n",
        "        return reconstructed_images, latent_representations\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, save_folder=\".\"):\n",
        "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(parameters_path, \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        autoencoder = VAE(*parameters)\n",
        "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        autoencoder.load_weights(weights_path)\n",
        "        return autoencoder\n",
        "\n",
        "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
        "                                                         + kl_loss\n",
        "        return combined_loss\n",
        "\n",
        "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "        error = y_target - y_predicted\n",
        "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                               K.exp(self.log_variance), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "    def _save_parameters(self, save_folder):\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.conv_filters,\n",
        "            self.conv_kernels,\n",
        "            self.conv_strides,\n",
        "            self.latent_space_dim\n",
        "        ]\n",
        "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, save_folder):\n",
        "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        self.model.save_weights(save_path)\n",
        "\n",
        "    def _build(self):\n",
        "        self._build_encoder()\n",
        "        self._build_decoder()\n",
        "        self._build_autoencoder()\n",
        "\n",
        "    def _build_autoencoder(self):\n",
        "        model_input = self._model_input\n",
        "        model_output = self.decoder(self.encoder(model_input))\n",
        "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "    def _build_decoder(self):\n",
        "        decoder_input = self._add_decoder_input()\n",
        "        dense_layer = self._add_dense_layer(decoder_input)\n",
        "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "    def _add_decoder_input(self):\n",
        "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "    def _add_dense_layer(self, decoder_input):\n",
        "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
        "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "        return dense_layer\n",
        "\n",
        "    def _add_reshape_layer(self, dense_layer):\n",
        "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "    def _add_conv_transpose_layers(self, x):\n",
        "        \"\"\"Add conv transpose blocks.\"\"\"\n",
        "        # loop through all the conv layers in reverse order and stop at the\n",
        "        # first layer\n",
        "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "            x = self._add_conv_transpose_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_transpose_layer(self, layer_index, x):\n",
        "        layer_num = self._num_conv_layers - layer_index\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_decoder_output(self, x):\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=1,\n",
        "            kernel_size=self.conv_kernels[0],\n",
        "            strides=self.conv_strides[0],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
        "        return output_layer\n",
        "\n",
        "    def _build_encoder(self):\n",
        "        encoder_input = self._add_encoder_input()\n",
        "        conv_layers = self._add_conv_layers(encoder_input)\n",
        "        bottleneck = self._add_bottleneck(conv_layers)\n",
        "        self._model_input = encoder_input\n",
        "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "    def _add_encoder_input(self):\n",
        "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "    def _add_conv_layers(self, encoder_input):\n",
        "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
        "        x = encoder_input\n",
        "        for layer_index in range(self._num_conv_layers):\n",
        "            x = self._add_conv_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_layer(self, layer_index, x):\n",
        "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
        "        conv 2d + ReLU + batch normalization.\n",
        "        \"\"\"\n",
        "        layer_number = layer_index + 1\n",
        "        conv_layer = Conv2D(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"encoder_conv_layer_{layer_number}\"\n",
        "        )\n",
        "        x = conv_layer(x)\n",
        "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_bottleneck(self, x):\n",
        "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense\n",
        "        layer).\n",
        "        \"\"\"\n",
        "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "        x = Flatten()(x)\n",
        "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
        "        self.log_variance = Dense(self.latent_space_dim,\n",
        "                                  name=\"log_variance\")(x)\n",
        "\n",
        "        def sample_point_from_normal_distribution(args):\n",
        "            mu, log_variance = args\n",
        "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
        "                                      stddev=1.)\n",
        "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "            return sampled_point\n",
        "\n",
        "        x = Lambda(sample_point_from_normal_distribution,\n",
        "                   name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    autoencoder = VAE(\n",
        "        input_shape=(28, 28, 1),\n",
        "        conv_filters=(32, 64, 64, 64),\n",
        "        conv_kernels=(3, 3, 3, 3),\n",
        "        conv_strides=(1, 2, 2, 1),\n",
        "        latent_space_dim=2\n",
        "    )\n",
        "    autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "Dpk7f0j8JCOb",
        "outputId": "7b48a885-ee37-42e0-e213-33a629965b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.__operators__.add/AddV2:0', description=\"created by layer 'tf.__operators__.add'\") of unsupported type <class 'keras.src.engine.keras_tensor.KerasTensor'>.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4b721f02a903>\u001b[0m in \u001b[0;36m<cell line: 228>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     autoencoder = VAE(\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mconv_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4b721f02a903>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4b721f02a903>\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4b721f02a903>\u001b[0m in \u001b[0;36m_build_encoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_encoder_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mconv_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_conv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_bottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4b721f02a903>\u001b[0m in \u001b[0;36m_add_bottleneck\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msampled_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         x = Lambda(sample_point_from_normal_distribution,\n\u001b[0m\u001b[1;32m    224\u001b[0m                    name=\"encoder_output\")([self.mu, self.log_variance])\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         3, \"Failed to convert %r to tensor: %s\" % (type(value).__name__, e))\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m   raise TypeError(f\"Could not build a TypeSpec for {value} of \"\n\u001b[0m\u001b[1;32m   1005\u001b[0m                   f\"unsupported type {type(value)}.\")\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.__operators__.add/AddV2:0', description=\"created by layer 'tf.__operators__.add'\") of unsupported type <class 'keras.src.engine.keras_tensor.KerasTensor'>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSYj0FHIJK1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}